{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import , division , print_function , unicode_literals\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dense , Activation\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"../trgc1000.csv\")\n",
    "y = pd.read_csv(\"../ffyrgc1000.csv\")\n",
    "x = x.iloc[:,1:]\n",
    "y = y.iloc[:,[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data set into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train ,y_test = train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750, 10332), (750, 1), (750, 1), (250, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape , y_train.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(10332,activation = \"relu\",input_shape = [len(x_train.keys())]),\n",
    "        #layers.Dense(5,activation = \"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    model.compile(\n",
    "    loss = \"mse\",\n",
    "    optimizer = optimizer,metrics = [\"mae\",\"mse\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now try out the Model. Take a batch of 10examples from the training data and call model.predict on it\n",
    "example_batch = np.array(x_train[:10])\n",
    "example_result = model.predict(example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_result*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model for 1000 epochs, and record the training and validation accuracy in the history object\n",
    "EPOCHS = 1000\n",
    "history = model.fit(np.array(x_train) , np.array(y_train), epochs=EPOCHS , validation_split=0.2,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist[\"epoch\"] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot({\"Basic\":history},metric= \"mae\")\n",
    "plt.ylim([0,10])\n",
    "plt.ylabel(\"MAE[FF]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot({\"Basic\":history},metric = \"mse\")\n",
    "plt.ylim([0,20])\n",
    "plt.ylabel(\"MSE[FFY2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets update the model.fit call to automatically stop training when the validation score doesn't improve\n",
    "WE 'll use an EarlyStopping callback that tests a training condition for every epoch. If a set amount of epochs elapses without showing improvement, then automatically stop the training\n",
    "More here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "# The patience Parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=10)\n",
    "\n",
    "#The patience parameter is the amount of epochs to check for improvement \n",
    "early_history = model.fit(np.array(x_train), np.array(y_train), epochs = EPOCHS, validation_split = 0.2, verbose= 0,\n",
    "                          callbacks = [early_stop, tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter.plot({\"Early Stopping\":early_history},metric = \"mae\")\n",
    "plt.ylim([0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss , mae , mse = model.evaluate(x_test, y_test , verbose = 2)\n",
    "print(\"Testing set Mean Abs Error:{:5.2f}MPG\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(np.array(x_test)).flatten()\n",
    "a = plt.axes(aspect = \"equal\")\n",
    "plt.scatter(y_test , test_predictions)\n",
    "plt.xlabel(\"True Values FFY\")\n",
    "plt.ylabel(\"Predictions FFY\")\n",
    "\n",
    "lims = [0,50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims,lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a look at the error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error =test_predictions - np.array(y_test)\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel(\"Prediction Error FFY\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook introduced a few techniques to handle a regression problem <br>\n",
    "* Mean Squared Error(MSE) is a common loss function used for regression problems (different loss functions are used for calssification problems)\n",
    "* Evaluation Metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE)\n",
    "* When Numeric input data features have values with different ranges, each feature should be scaled independently to the same range\n",
    "* If there is not much training data, one technique is to prefer a small network with few hidden layers to avoid overfittintg.\n",
    "* Early stopping is a useful technique to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = np.array(x)\n",
    "y_ = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10332)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "1000/1000 [==============================] - 0s 307us/step - loss: 282.4512\n",
      "Epoch 2/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 9.2648\n",
      "Epoch 3/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.4159\n",
      "Epoch 4/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0657\n",
      "Epoch 5/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0391\n",
      "Epoch 6/120\n",
      "1000/1000 [==============================] - 0s 263us/step - loss: 0.0397\n",
      "Epoch 7/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.0369\n",
      "Epoch 8/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0360\n",
      "Epoch 9/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.0408\n",
      "Epoch 10/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0403\n",
      "Epoch 11/120\n",
      "1000/1000 [==============================] - 0s 260us/step - loss: 0.0393\n",
      "Epoch 12/120\n",
      "1000/1000 [==============================] - 0s 271us/step - loss: 0.0310\n",
      "Epoch 13/120\n",
      "1000/1000 [==============================] - 0s 260us/step - loss: 0.0313\n",
      "Epoch 14/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0281\n",
      "Epoch 15/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0322\n",
      "Epoch 16/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.0272\n",
      "Epoch 17/120\n",
      "1000/1000 [==============================] - 0s 265us/step - loss: 0.0320\n",
      "Epoch 18/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.0267\n",
      "Epoch 19/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.0315\n",
      "Epoch 20/120\n",
      "1000/1000 [==============================] - 0s 267us/step - loss: 0.0278\n",
      "Epoch 21/120\n",
      "1000/1000 [==============================] - 0s 271us/step - loss: 0.0308\n",
      "Epoch 22/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0212\n",
      "Epoch 23/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0233\n",
      "Epoch 24/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0274\n",
      "Epoch 25/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0258\n",
      "Epoch 26/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0166\n",
      "Epoch 27/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.0229\n",
      "Epoch 28/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0345\n",
      "Epoch 29/120\n",
      "1000/1000 [==============================] - 0s 255us/step - loss: 0.0200\n",
      "Epoch 30/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0231\n",
      "Epoch 31/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0195\n",
      "Epoch 32/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.0203\n",
      "Epoch 33/120\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.0193\n",
      "Epoch 34/120\n",
      "1000/1000 [==============================] - 0s 269us/step - loss: 0.0170\n",
      "Epoch 35/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0130\n",
      "Epoch 36/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0165\n",
      "Epoch 37/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0158\n",
      "Epoch 38/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0166\n",
      "Epoch 39/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0167\n",
      "Epoch 40/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0202\n",
      "Epoch 41/120\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.0246\n",
      "Epoch 42/120\n",
      "1000/1000 [==============================] - 0s 273us/step - loss: 0.0408\n",
      "Epoch 43/120\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.0203\n",
      "Epoch 44/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.0304\n",
      "Epoch 45/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0217\n",
      "Epoch 46/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0284\n",
      "Epoch 47/120\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.0178\n",
      "Epoch 48/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0281\n",
      "Epoch 49/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.0204\n",
      "Epoch 50/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0345\n",
      "Epoch 51/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.1382\n",
      "Epoch 52/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.0477\n",
      "Epoch 53/120\n",
      "1000/1000 [==============================] - 0s 262us/step - loss: 0.0869\n",
      "Epoch 54/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.1074\n",
      "Epoch 55/120\n",
      "1000/1000 [==============================] - 0s 254us/step - loss: 0.3549\n",
      "Epoch 56/120\n",
      "1000/1000 [==============================] - 0s 254us/step - loss: 0.5287\n",
      "Epoch 57/120\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.6604\n",
      "Epoch 58/120\n",
      "1000/1000 [==============================] - 0s 273us/step - loss: 0.7588\n",
      "Epoch 59/120\n",
      "1000/1000 [==============================] - 0s 264us/step - loss: 0.8301\n",
      "Epoch 60/120\n",
      "1000/1000 [==============================] - 0s 263us/step - loss: 2.9693\n",
      "Epoch 61/120\n",
      "1000/1000 [==============================] - 0s 275us/step - loss: 0.2447\n",
      "Epoch 62/120\n",
      "1000/1000 [==============================] - 0s 266us/step - loss: 0.5728\n",
      "Epoch 63/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.8989\n",
      "Epoch 64/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 2.6012\n",
      "Epoch 65/120\n",
      "1000/1000 [==============================] - 0s 255us/step - loss: 0.7234\n",
      "Epoch 66/120\n",
      "1000/1000 [==============================] - 0s 255us/step - loss: 0.2503\n",
      "Epoch 67/120\n",
      "1000/1000 [==============================] - 0s 263us/step - loss: 0.3005\n",
      "Epoch 68/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 2.2620\n",
      "Epoch 69/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.1712\n",
      "Epoch 70/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.1566\n",
      "Epoch 71/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 1.8440\n",
      "Epoch 72/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.3443\n",
      "Epoch 73/120\n",
      "1000/1000 [==============================] - 0s 263us/step - loss: 0.0775\n",
      "Epoch 74/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 7.2395\n",
      "Epoch 75/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 1.4214\n",
      "Epoch 76/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0599\n",
      "Epoch 77/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0313\n",
      "Epoch 78/120\n",
      "1000/1000 [==============================] - 0s 255us/step - loss: 0.0451\n",
      "Epoch 79/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0370\n",
      "Epoch 80/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0556\n",
      "Epoch 81/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.0961\n",
      "Epoch 82/120\n",
      "1000/1000 [==============================] - 0s 266us/step - loss: 0.0733\n",
      "Epoch 83/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.3874\n",
      "Epoch 84/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.3879\n",
      "Epoch 85/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.5641\n",
      "Epoch 86/120\n",
      "1000/1000 [==============================] - 0s 260us/step - loss: 0.3547\n",
      "Epoch 87/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 1.5529\n",
      "Epoch 88/120\n",
      "1000/1000 [==============================] - 0s 263us/step - loss: 0.3917\n",
      "Epoch 89/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.3629\n",
      "Epoch 90/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.4370\n",
      "Epoch 91/120\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 0.2782\n",
      "Epoch 92/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 2.2487\n",
      "Epoch 93/120\n",
      "1000/1000 [==============================] - 0s 269us/step - loss: 0.4745\n",
      "Epoch 94/120\n",
      "1000/1000 [==============================] - 0s 269us/step - loss: 0.0977\n",
      "Epoch 95/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.1567\n",
      "Epoch 96/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.1652\n",
      "Epoch 97/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.8417\n",
      "Epoch 98/120\n",
      "1000/1000 [==============================] - 0s 271us/step - loss: 1.2346\n",
      "Epoch 99/120\n",
      "1000/1000 [==============================] - 0s 265us/step - loss: 0.0694\n",
      "Epoch 100/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.1503\n",
      "Epoch 101/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.1743\n",
      "Epoch 102/120\n",
      "1000/1000 [==============================] - 0s 265us/step - loss: 0.2753\n",
      "Epoch 103/120\n",
      "1000/1000 [==============================] - 0s 260us/step - loss: 1.4084\n",
      "Epoch 104/120\n",
      "1000/1000 [==============================] - 0s 255us/step - loss: 0.1833\n",
      "Epoch 105/120\n",
      "1000/1000 [==============================] - 0s 257us/step - loss: 0.0852\n",
      "Epoch 106/120\n",
      "1000/1000 [==============================] - 0s 253us/step - loss: 0.0853\n",
      "Epoch 107/120\n",
      "1000/1000 [==============================] - 0s 254us/step - loss: 2.5368\n",
      "Epoch 108/120\n",
      "1000/1000 [==============================] - 0s 259us/step - loss: 0.2196\n",
      "Epoch 109/120\n",
      "1000/1000 [==============================] - 0s 276us/step - loss: 0.0789\n",
      "Epoch 110/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.1555\n",
      "Epoch 111/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.1785\n",
      "Epoch 112/120\n",
      "1000/1000 [==============================] - 0s 263us/step - loss: 0.0407\n",
      "Epoch 113/120\n",
      "1000/1000 [==============================] - 0s 258us/step - loss: 0.0784\n",
      "Epoch 114/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.4167\n",
      "Epoch 115/120\n",
      "1000/1000 [==============================] - 0s 273us/step - loss: 0.3922\n",
      "Epoch 116/120\n",
      "1000/1000 [==============================] - 0s 275us/step - loss: 0.1410\n",
      "Epoch 117/120\n",
      "1000/1000 [==============================] - 0s 267us/step - loss: 0.6590\n",
      "Epoch 118/120\n",
      "1000/1000 [==============================] - 0s 261us/step - loss: 0.2582\n",
      "Epoch 119/120\n",
      "1000/1000 [==============================] - 0s 260us/step - loss: 0.2709\n",
      "Epoch 120/120\n",
      "1000/1000 [==============================] - 0s 262us/step - loss: 0.1313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x203c0e03248>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "model =Sequential()\n",
    "model.add(Dense(32,input_shape= (10332,)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer = \"adam\", loss = \"mean_squared_error\"  )\n",
    "\n",
    "# This Builds the model for the first time\n",
    "model.fit(x_,y_,batch_size = 32, epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "#model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.551289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.632919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.518726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.428922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.592852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.588358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.653202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.644636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.531708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.635265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.551289\n",
       "1  0.632919\n",
       "2  0.518726\n",
       "3  0.428922\n",
       "4  0.592852\n",
       "5  0.588358\n",
       "6  0.653202\n",
       "7  0.644636\n",
       "8  0.531708\n",
       "9  0.635265"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model.predict(np.array(x_test))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.125959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.186891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>0.059090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>0.048027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.091914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           V1\n",
       "313  0.125959\n",
       "8    0.186891\n",
       "698  0.059090\n",
       "699  0.048027\n",
       "251  0.091914"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda67970ada4b5d499c879aef5e70dc73ee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
